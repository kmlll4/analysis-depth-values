{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bisect\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "\n",
    "DIR = '/workspace/pig'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m_df = pd.read_pickle(os.path.join(DIR, 'data', \"merged_df_20211101.pkl\"))\n",
    "m_df = pd.read_pickle(\"../tmp.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add bin column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [30., 40., 50., 60., 70., 80., 90., 100., 110.]\n",
    "bins = bins + [float(i) for i in range(111, 125)] + [125., 130., 140.]\n",
    "labels = [p.weight for idx, p in m_df.iterrows()]\n",
    "_, bins = np.histogram(labels, range=range, bins=bins)\n",
    "m_df['bins'] = [bisect.bisect_right(bins, label) for label in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split train val test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sorted(list(m_df['bins'].value_counts().index)):\n",
    "    idxes = list(set(m_df[m_df['bins'] == i]['UID']))\n",
    "    \n",
    "    if len(idxes) <= 3:\n",
    "        continue\n",
    "    \n",
    "    # split train and val_test\n",
    "    if len(m_df[m_df['bins'] == i])*0.3 >= 200:\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=200 / m_df[m_df['bins'] == i].shape[0], random_state=0)\n",
    "    else:\n",
    "        gss = GroupShuffleSplit(n_splits=1, train_size=0.7, random_state=0)\n",
    "    gss.get_n_splits()\n",
    "    bin_df = m_df[m_df['bins'] == i]\n",
    "\n",
    "    idxes = np.array(bin_df.index)\n",
    "    groups =  list(bin_df['UID'])\n",
    "\n",
    "    train_idx, val_test_idx = gss.split(idxes, idxes, groups).__next__()\n",
    "    assert len(set(bin_df.iloc[train_idx]['UID']) & set(bin_df.iloc[val_test_idx]['UID'])) == 0\n",
    "\n",
    "    # split val and test\n",
    "    gss = GroupShuffleSplit(n_splits=1, train_size=0.5, random_state=0)\n",
    "    gss.get_n_splits()\n",
    "    groups =  list(bin_df.iloc[val_test_idx]['UID'])\n",
    "\n",
    "    _val_idx, _test_idx = gss.split(val_test_idx, val_test_idx, groups).__next__()\n",
    "    val_idx = val_test_idx[_val_idx]\n",
    "    test_idx = val_test_idx[_test_idx]\n",
    "\n",
    "    assert len(set(bin_df.iloc[train_idx]['UID']) & set(bin_df.iloc[val_idx]['UID']) & set(bin_df.iloc[test_idx]['UID'])) == 0\n",
    "    \n",
    "    print(bins[i])\n",
    "    print(len(set(bin_df.iloc[train_idx]['UID'])))\n",
    "    print(len(set(bin_df.iloc[val_idx]['UID'])))\n",
    "    print(len(set(bin_df.iloc[test_idx]['UID'])))\n",
    "    print()\n",
    "\n",
    "    m_df.loc[idxes[train_idx], 'subset'] = 'train'\n",
    "    m_df.loc[idxes[val_idx], 'subset'] = 'val'\n",
    "    m_df.loc[idxes[test_idx], 'subset'] = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count, bins = np.histogram(\n",
    "    [p.weight for idx, p in m_df[m_df['subset'] == 'train'].iterrows()], bins=bins)\n",
    "print(m_df[m_df['subset'] == 'train'].shape)\n",
    "print(\"bins: \", bins)\n",
    "print(\"count: \", count)\n",
    "print()\n",
    "\n",
    "count, bins = np.histogram(\n",
    "    [p.weight for idx, p in m_df[m_df['subset'] == 'val'].iterrows()], bins=bins)\n",
    "print(m_df[m_df['subset'] == 'val'].shape)\n",
    "print(\"bins: \", bins)\n",
    "print(\"count: \", count)\n",
    "print()\n",
    "\n",
    "count, bins = np.histogram(\n",
    "    [p.weight for idx, p in m_df[m_df['subset'] == 'test'].iterrows()], bins=bins)\n",
    "print(m_df[m_df['subset'] == 'test'].shape)\n",
    "print(\"bins: \", bins)\n",
    "print(\"count: \", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_df.to_pickle(os.path.join(DIR, 'data/akiyama_20211210.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_pickle(os.path.join(DIR, 'data/akiyama_20211202.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
